{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning - MNIST - Role: Data Scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook to be developed by the *data scientist*. \n",
    " * As a Data Scientist and you do not know where data lives, you only have access to GridNetwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal: Train a remote Deep Learning model**\n",
    "\n",
    "In this notebbok, we will show how to train a Federated Deep Learning with data hosted in Nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Previous setup\n",
    "\n",
    "Components:\n",
    "\n",
    " - PyGrid Network      http://alice:7000\n",
    " - PyGrid Node Alice (http://bob:5000)\n",
    " - PyGrid Node Bob   (http://charlie:5001)\n",
    "\n",
    "This tutorial assumes that these components are running in background. See [instructions](https://github.com/OpenMined/PyGrid/tree/dev/examples#how-to-run-this-tutorial) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "Here we import core dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.grid.public_grid import PublicGridNetwork\n",
    "\n",
    "import torch as th\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syft and client configuration\n",
    "Now we hook Torch and connect to the GridNetwork. This is the only sever you do not need to know node addresses (networks knows), but lets first define some useful parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "grid_address = \"http://0.0.0.0:7000\"  # address\n",
    "N_EPOCHS = 100  # number of epochs to train\n",
    "N_TEST   = 10   # number of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(th)\n",
    "\n",
    "\n",
    "# Connect direcly to grid nodes\n",
    "my_grid = PublicGridNetwork(hook, grid_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Define our Neural Network Arquitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a Deep Learning Network, feel free to write your own model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.test_batch_size = N_TEST\n",
    "        self.epochs = N_EPOCHS\n",
    "        self.lr = 0.01\n",
    "        self.log_interval = 5\n",
    "        self.device = th.device(\"cpu\")\n",
    "        \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "model = Net()\n",
    "model.to(args.device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Search for remote data\n",
    "\n",
    "Once we have defined our Deep Learning Network, we need some data to train... Thanks to PyGridNetwork this is very easy, you just need to search for your tags of interest.\n",
    "\n",
    "Notice that _search()_ method  returns a pointer tensor, so we will work with those keeping real tensors hosted in Alice and Bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = my_grid.search(\"#X\", \"#mnist\", \"#dataset\")  # images\n",
    "target = my_grid.search(\"#Y\", \"#mnist\", \"#dataset\")  # labels\n",
    "\n",
    "data = list(data.values())  # returns a pointer\n",
    "target = list(target.values())  # returns a pointer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print the tensors, we can check how the metadata we added before is included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(Wrapper)>[PointerTensor | me:84975565974 -> bob:61788994027]\n",
      "\tTags: #X #dataset #mnist \n",
      "\tShape: torch.Size([5000, 1, 28, 28])\n",
      "\tDescription: The input datapoints to the MNIST dataset....], [(Wrapper)>[PointerTensor | me:44971294991 -> alice:31469484730]\n",
      "\tTags: #X #dataset #mnist \n",
      "\tShape: torch.Size([5000, 1, 28, 28])\n",
      "\tDescription: The input datapoints to the MNIST dataset....]]\n",
      "[[(Wrapper)>[PointerTensor | me:78421685917 -> bob:90201928853]\n",
      "\tTags: #dataset #mnist #Y \n",
      "\tShape: torch.Size([5000])\n",
      "\tDescription: The input labels to the MNIST dataset....], [(Wrapper)>[PointerTensor | me:38922590983 -> alice:3024702016]\n",
      "\tTags: #dataset #mnist #Y \n",
      "\tShape: torch.Size([5000])\n",
      "\tDescription: The input labels to the MNIST dataset....]]\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train. As you will see, this is very similar to standard pytorch sintax.\n",
    "\n",
    "Let's first load test data in order to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,)),  #  mean and std \n",
    "                              ])\n",
    "testset = datasets.MNIST('./dataset', download=False, train=False, transform=transform)\n",
    "testloader = th.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch size\n",
    "def epoch_total_size(data):\n",
    "    total = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            total += data[i][j].shape[0]\n",
    "            \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_total = epoch_total_size(data)\n",
    "    \n",
    "    current_epoch_size = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            \n",
    "            current_epoch_size += len(data[i][j])\n",
    "            worker = data[i][j].location  # worker hosts data\n",
    "            \n",
    "            model.send(worker)  # send model to PyGridNode worker\n",
    "            optimizer.zero_grad()  \n",
    "            \n",
    "            pred = model(data[i][j])\n",
    "            loss = F.nll_loss(pred, target[i][j])\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            model.get()  # get back the model\n",
    "            \n",
    "            loss = loss.get()\n",
    "            \n",
    "        if epoch % args.log_interval == 0:\n",
    "\n",
    "            print('Train Epoch: {} | With {} data |: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                      epoch, worker.id, current_epoch_size, epoch_total,\n",
    "                            100. *  current_epoch_size / epoch_total, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args):\n",
    "    \n",
    "    if epoch % args.log_interval == 0:\n",
    "    \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in testloader:\n",
    "                data, target = data.to(args.device), target.to(args.device)\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "                pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(testloader.dataset)\n",
    "\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(testloader.dataset),\n",
    "            100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 | With bob data |: [5000/10000 (50%)]\tLoss: 2.307125\n",
      "Train Epoch: 0 | With alice data |: [10000/10000 (100%)]\tLoss: 2.302680\n",
      "\n",
      "Test set: Average loss: 2.2972, Accuracy: 914/10000 (9%)\n",
      "\n",
      "Train Epoch: 5 | With bob data |: [5000/10000 (50%)]\tLoss: 2.262557\n",
      "Train Epoch: 5 | With alice data |: [10000/10000 (100%)]\tLoss: 2.258535\n",
      "\n",
      "Test set: Average loss: 2.2522, Accuracy: 3600/10000 (36%)\n",
      "\n",
      "Train Epoch: 10 | With bob data |: [5000/10000 (50%)]\tLoss: 2.216732\n",
      "Train Epoch: 10 | With alice data |: [10000/10000 (100%)]\tLoss: 2.212326\n",
      "\n",
      "Test set: Average loss: 2.2045, Accuracy: 5554/10000 (56%)\n",
      "\n",
      "Train Epoch: 15 | With bob data |: [5000/10000 (50%)]\tLoss: 2.162978\n",
      "Train Epoch: 15 | With alice data |: [10000/10000 (100%)]\tLoss: 2.157516\n",
      "\n",
      "Test set: Average loss: 2.1474, Accuracy: 6619/10000 (66%)\n",
      "\n",
      "Train Epoch: 20 | With bob data |: [5000/10000 (50%)]\tLoss: 2.093618\n",
      "Train Epoch: 20 | With alice data |: [10000/10000 (100%)]\tLoss: 2.086318\n",
      "\n",
      "Test set: Average loss: 2.0727, Accuracy: 6958/10000 (70%)\n",
      "\n",
      "Train Epoch: 25 | With bob data |: [5000/10000 (50%)]\tLoss: 1.999349\n",
      "Train Epoch: 25 | With alice data |: [10000/10000 (100%)]\tLoss: 1.989205\n",
      "\n",
      "Test set: Average loss: 1.9705, Accuracy: 7162/10000 (72%)\n",
      "\n",
      "Train Epoch: 30 | With bob data |: [5000/10000 (50%)]\tLoss: 1.869296\n",
      "Train Epoch: 30 | With alice data |: [10000/10000 (100%)]\tLoss: 1.854985\n",
      "\n",
      "Test set: Average loss: 1.8295, Accuracy: 7324/10000 (73%)\n",
      "\n",
      "Train Epoch: 35 | With bob data |: [5000/10000 (50%)]\tLoss: 1.695734\n",
      "Train Epoch: 35 | With alice data |: [10000/10000 (100%)]\tLoss: 1.676666\n",
      "\n",
      "Test set: Average loss: 1.6436, Accuracy: 7525/10000 (75%)\n",
      "\n",
      "Train Epoch: 40 | With bob data |: [5000/10000 (50%)]\tLoss: 1.485063\n",
      "Train Epoch: 40 | With alice data |: [10000/10000 (100%)]\tLoss: 1.461614\n",
      "\n",
      "Test set: Average loss: 1.4222, Accuracy: 7808/10000 (78%)\n",
      "\n",
      "Train Epoch: 45 | With bob data |: [5000/10000 (50%)]\tLoss: 1.260913\n",
      "Train Epoch: 45 | With alice data |: [10000/10000 (100%)]\tLoss: 1.235452\n",
      "\n",
      "Test set: Average loss: 1.1936, Accuracy: 8085/10000 (81%)\n",
      "\n",
      "Train Epoch: 50 | With bob data |: [5000/10000 (50%)]\tLoss: 1.057105\n",
      "Train Epoch: 50 | With alice data |: [10000/10000 (100%)]\tLoss: 1.032679\n",
      "\n",
      "Test set: Average loss: 0.9922, Accuracy: 8234/10000 (82%)\n",
      "\n",
      "Train Epoch: 55 | With bob data |: [5000/10000 (50%)]\tLoss: 0.894551\n",
      "Train Epoch: 55 | With alice data |: [10000/10000 (100%)]\tLoss: 0.872867\n",
      "\n",
      "Test set: Average loss: 0.8359, Accuracy: 8344/10000 (83%)\n",
      "\n",
      "Train Epoch: 60 | With bob data |: [5000/10000 (50%)]\tLoss: 0.775114\n",
      "Train Epoch: 60 | With alice data |: [10000/10000 (100%)]\tLoss: 0.756199\n",
      "\n",
      "Test set: Average loss: 0.7228, Accuracy: 8449/10000 (84%)\n",
      "\n",
      "Train Epoch: 65 | With bob data |: [5000/10000 (50%)]\tLoss: 0.689549\n",
      "Train Epoch: 65 | With alice data |: [10000/10000 (100%)]\tLoss: 0.672690\n",
      "\n",
      "Test set: Average loss: 0.6424, Accuracy: 8501/10000 (85%)\n",
      "\n",
      "Train Epoch: 70 | With bob data |: [5000/10000 (50%)]\tLoss: 0.628263\n",
      "Train Epoch: 70 | With alice data |: [10000/10000 (100%)]\tLoss: 0.612740\n",
      "\n",
      "Test set: Average loss: 0.5853, Accuracy: 8565/10000 (86%)\n",
      "\n",
      "Train Epoch: 75 | With bob data |: [5000/10000 (50%)]\tLoss: 0.585702\n",
      "Train Epoch: 75 | With alice data |: [10000/10000 (100%)]\tLoss: 0.570367\n",
      "\n",
      "Test set: Average loss: 0.5468, Accuracy: 8589/10000 (86%)\n",
      "\n",
      "Train Epoch: 80 | With bob data |: [5000/10000 (50%)]\tLoss: 0.558290\n",
      "Train Epoch: 80 | With alice data |: [10000/10000 (100%)]\tLoss: 0.540569\n",
      "\n",
      "Test set: Average loss: 0.5222, Accuracy: 8583/10000 (86%)\n",
      "\n",
      "Train Epoch: 85 | With bob data |: [5000/10000 (50%)]\tLoss: 0.535635\n",
      "Train Epoch: 85 | With alice data |: [10000/10000 (100%)]\tLoss: 0.515276\n",
      "\n",
      "Test set: Average loss: 0.4998, Accuracy: 8609/10000 (86%)\n",
      "\n",
      "Train Epoch: 90 | With bob data |: [5000/10000 (50%)]\tLoss: 0.511364\n",
      "Train Epoch: 90 | With alice data |: [10000/10000 (100%)]\tLoss: 0.491225\n",
      "\n",
      "Test set: Average loss: 0.4756, Accuracy: 8667/10000 (87%)\n",
      "\n",
      "Train Epoch: 95 | With bob data |: [5000/10000 (50%)]\tLoss: 0.487891\n",
      "Train Epoch: 95 | With alice data |: [10000/10000 (100%)]\tLoss: 0.469482\n",
      "\n",
      "Test set: Average loss: 0.4524, Accuracy: 8726/10000 (87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    train(args)\n",
    "    test(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! Here you are, you have trained a model on remote data using Federated Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the reference example of [PyGrid Network Data-Centric](https://github.com/OpenMined/PyGrid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
