{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning - MNIST - Role: Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an auxiliar notebook to populate the network. Here we introduce an extra role: the Data Loader who is responsible to populate the NetworkGrid. \n",
    "\n",
    "*Important note: This notebook is only for testing purposes. In a real scenario, the data is already on the network.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal: Populate remote GridNodes with labeled tensors**\n",
    "\n",
    "In this notebbok, we will show how to populate a GridNode with labeled data, so it will be used later (link to second part) by people interested in train models.\n",
    "\n",
    "In particular, we will consider that two Data Owners (Alice & Bob) want to populate their nodes with some data from the well-known MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Previous setup\n",
    "\n",
    "Components:\n",
    "\n",
    " - PyGrid Network      http://network:7000\n",
    " - PyGrid Node Alice (http://alice:5000)\n",
    " - PyGrid Node Bob   (http://bob:5001)\n",
    "\n",
    "This tutorial assumes that these components are running in background. See [instructions](https://github.com/OpenMined/PyGrid/tree/dev/examples#how-to-run-this-tutorial) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "Here we import core dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.grid.clients.data_centric_fl_client import DataCentricFLClient  # websocket client. It sends commands to the node servers\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# we create some dicts to store losses\n",
    "torch_loss_stats = {\n",
    "    'torch loss train': [],\n",
    "    'torch loss val': []\n",
    "}\n",
    "\n",
    "syft_loss_stats = {\n",
    "    'syft loss train': [],\n",
    "    'syft loss val': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syft and client configuration\n",
    "Now we hook Torch and connect the clients to the servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# address\n",
    "alice_address = \"http://0.0.0.0:5001\" \n",
    "bob_address   = \"http://0.0.0.0:5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Alice connected?: True\n",
      "Is Bob connected?: True\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Connect direcly to grid nodes\n",
    "compute_nodes = {}\n",
    "\n",
    "compute_nodes[\"Alice\"] = DataCentricFLClient(hook, alice_address)\n",
    "compute_nodes[\"Bob\"]   = DataCentricFLClient(hook, bob_address) \n",
    "\n",
    "# Check if they are connected\n",
    "for key, value in compute_nodes.items(): \n",
    "    print(\"Is \" + key + \" connected?: \" + str(value.ws.connected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load dataset\n",
    "Download (and load) the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 10000  # Number of samples\n",
    "MNIST_PATH = './dataset'  # Path to save MNIST dataset\n",
    "\n",
    "# Define a transformation.\n",
    "transform = transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,)),  #  mean and std \n",
    "                              ])\n",
    "\n",
    "# Download and load MNIST dataset\n",
    "trainset = datasets.MNIST(MNIST_PATH, download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=N_SAMPLES, shuffle=True)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images_train_mnist, labels_train_mnist = dataiter.next()  # Train images and their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Split dataset\n",
    "We split our dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train_mnist = torch.split(images_train_mnist, int(len(images_train_mnist) / len(compute_nodes)), dim=0 ) #tuple of chunks (dataset / number of nodes)\n",
    "labels_train_mnist   = torch.split(labels_train_mnist, int(len(labels_train_mnist) / len(compute_nodes)), dim=0 )  #tuple of chunks (labels / number of nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we add tags to them so that we can search them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, _ in enumerate(compute_nodes):\n",
    "        \n",
    "    images_train_mnist[index]\\\n",
    "        .tag(\"#X\", \"#mnist\", \"#dataset\")\\\n",
    "        .describe(\"The input datapoints to the MNIST dataset.\") \n",
    "    \n",
    "    \n",
    "    labels_train_mnist[index]\\\n",
    "        .tag(\"#Y\", \"#mnist\", \"#dataset\") \\\n",
    "        .describe(\"The input labels to the MNIST dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Sending our tensor to grid nodes\n",
    "\n",
    "We can consider the previous steps as data preparation, i.e., in a more realistic scenario Alice and Bob would already have their data, so they just would need to load their tensors into their nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending data to Alice\n",
      "Sending data to Bob\n"
     ]
    }
   ],
   "source": [
    "for index, key in enumerate(compute_nodes):\n",
    "    \n",
    "    print(\"Sending data to\", key)\n",
    "    \n",
    "    images_train_mnist[index].send(compute_nodes[key], garbage_collect_data=False)\n",
    "    labels_train_mnist[index].send(compute_nodes[key], garbage_collect_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is ok, tensors must be hosted in the nodes. GridNode have a specific endpoint to request what tensors are hosted. Let's check it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's tags:  ['#Y', '#dataset', '#mnist', '#X']\n",
      "Bob's tags:  ['#dataset', '#mnist', '#Y', '#X']\n"
     ]
    }
   ],
   "source": [
    "print(\"Alice's tags: \", requests.get(alice_address + \"/data-centric/dataset-tags\").json())\n",
    "print(\"Bob's tags: \",   requests.get(bob_address   + \"/data-centric/dataset-tags\").json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the reference example of [PyGrid Network Data-Centric](https://github.com/OpenMined/PyGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
