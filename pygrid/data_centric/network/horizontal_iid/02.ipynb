{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning - MNIST Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a remote Deep Learning model\n",
    "In this notebbok, we will show how to train a Federated Deep Learning with data hosted in Nodes.\n",
    "\n",
    "We will consider that you are a Data Scientist and you do not know where data lives, you only have access to GridNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Previous setup\n",
    "\n",
    "Components:\n",
    "\n",
    " - PyGrid Network      http://alice:7000\n",
    " - PyGrid Node Alice (http://bob:5000)\n",
    " - PyGrid Node Bob   (http://charlie:5001)\n",
    "\n",
    "This tutorial assumes that these components are running in background. See [instructions](https://github.com/OpenMined/PyGrid/tree/dev/examples#how-to-run-this-tutorial) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "Here we import core dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.grid.public_grid import PublicGridNetwork\n",
    "\n",
    "import torch as th\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syft and client configuration\n",
    "Now we hook Torch and connect to the GridNetwork. This is the only sever you do not need to know node addresses (networks knows), but lets first define some useful parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "grid_address = \"http://0.0.0.0:7000\"  # address\n",
    "N_EPOCHS = 100  # number of epochs to train\n",
    "N_TEST   = 10   # number of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(th)\n",
    "\n",
    "\n",
    "# Connect direcly to grid nodes\n",
    "my_grid = PublicGridNetwork(hook, grid_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Define our Neural Network Arquitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a Deep Learning Network, feel free to write your own model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.test_batch_size = N_TEST\n",
    "        self.epochs = N_EPOCHS\n",
    "        self.lr = 0.01\n",
    "        self.log_interval = 5\n",
    "        self.device = th.device(\"cpu\")\n",
    "        \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "model = Net()\n",
    "model.to(args.device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Search for remote data\n",
    "\n",
    "Once we have defined our Deep Learning Network, we need some data to train... Thanks to PyGridNetwork this is very easy, you just need to search for your tags of interest.\n",
    "\n",
    "Notice that _search()_ method  returns a pointer tensor, so we will work with those keeping real tensors hosted in Alice and Bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = my_grid.search(\"#X\", \"#mnist\", \"#dataset\")  # images\n",
    "target = my_grid.search(\"#Y\", \"#mnist\", \"#dataset\")  # labels\n",
    "\n",
    "data = list(data.values())  # returns a pointer\n",
    "target = list(target.values())  # returns a pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bob': [(Wrapper)>[PointerTensor | me:96688400479 -> alice:16153059467]\n",
       "  \tTags: #dataset #X #mnist \n",
       "  \tShape: torch.Size([5000, 1, 28, 28])\n",
       "  \tDescription: The input datapoints to the MNIST dataset....,\n",
       "  (Wrapper)>[PointerTensor | me:98840471352 -> alice:59692266119]\n",
       "  \tTags: #dataset #X #mnist \n",
       "  \tShape: torch.Size([5000, 1, 28, 28])\n",
       "  \tDescription: The input datapoints to the MNIST dataset....],\n",
       " 'alice': [(Wrapper)>[PointerTensor | me:97941167099 -> alice:16153059467]\n",
       "  \tTags: #dataset #X #mnist \n",
       "  \tShape: torch.Size([5000, 1, 28, 28])\n",
       "  \tDescription: The input datapoints to the MNIST dataset....,\n",
       "  (Wrapper)>[PointerTensor | me:31888220561 -> alice:59692266119]\n",
       "  \tTags: #dataset #X #mnist \n",
       "  \tShape: torch.Size([5000, 1, 28, 28])\n",
       "  \tDescription: The input datapoints to the MNIST dataset....]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "import syft as sy\n",
    "from syft.grid.public_grid import PublicGridNetwork\n",
    "from syft.grid.clients.data_centric_fl_client import DataCentricFLClient\n",
    "hook = sy.TorchHook(th)\n",
    "my_grid = PublicGridNetwork(hook, \"http://0.0.0.0:7000\")\n",
    "my_grid.search(\"#X\", \"#mnist\", \"#dataset\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print the tensors, we can check how the metadata we added before is included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(Wrapper)>[PointerTensor | me:41363926715 -> alice:16153059467]\n",
      "\tTags: #mnist #X #dataset \n",
      "\tShape: torch.Size([5000, 1, 28, 28])\n",
      "\tDescription: The input datapoints to the MNIST dataset....], [(Wrapper)>[PointerTensor | me:14266248798 -> alice:16153059467]\n",
      "\tTags: #mnist #X #dataset \n",
      "\tShape: torch.Size([5000, 1, 28, 28])\n",
      "\tDescription: The input datapoints to the MNIST dataset....]]\n",
      "[[(Wrapper)>[PointerTensor | me:80816069829 -> alice:25252518761]\n",
      "\tTags: #Y #mnist #dataset \n",
      "\tShape: torch.Size([5000])\n",
      "\tDescription: The input labels to the MNIST dataset....], [(Wrapper)>[PointerTensor | me:80111819498 -> alice:25252518761]\n",
      "\tTags: #Y #mnist #dataset \n",
      "\tShape: torch.Size([5000])\n",
      "\tDescription: The input labels to the MNIST dataset....]]\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train. As you will see, this is very similar to standard pytorch sintax.\n",
    "\n",
    "Let's first load test data in order to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,)),  #  mean and std \n",
    "                              ])\n",
    "testset = datasets.MNIST('./dataset', download=False, train=False, transform=transform)\n",
    "testloader = th.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch size\n",
    "def epoch_total_size(data):\n",
    "    total = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            total += data[i][j].shape[0]\n",
    "            \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_total = epoch_total_size(data)\n",
    "    \n",
    "    current_epoch_size = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            \n",
    "            current_epoch_size += len(data[i][j])\n",
    "            worker = data[i][j].location  # worker hosts data\n",
    "            \n",
    "            model.send(worker)  # send model to PyGridNode worker\n",
    "            optimizer.zero_grad()  \n",
    "            \n",
    "            pred = model(data[i][j])\n",
    "            loss = F.nll_loss(pred, target[i][j])\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            model.get()  # get back the model\n",
    "            \n",
    "            loss = loss.get()\n",
    "            \n",
    "        if epoch % args.log_interval == 0:\n",
    "\n",
    "            print('Train Epoch: {} | With {} data |: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                      epoch, worker.id, current_epoch_size, epoch_total,\n",
    "                            100. *  current_epoch_size / epoch_total, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args):\n",
    "    \n",
    "    if epoch % args.log_interval == 0:\n",
    "    \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in testloader:\n",
    "                data, target = data.to(args.device), target.to(args.device)\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "                pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(testloader.dataset)\n",
    "\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(testloader.dataset),\n",
    "            100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 | With alice data |: [5000/10000 (50%)]\tLoss: 2.307506\n",
      "Train Epoch: 0 | With alice data |: [10000/10000 (100%)]\tLoss: 2.304010\n",
      "\n",
      "Test set: Average loss: 2.3020, Accuracy: 896/10000 (9%)\n",
      "\n",
      "Train Epoch: 5 | With alice data |: [5000/10000 (50%)]\tLoss: 2.273522\n",
      "Train Epoch: 5 | With alice data |: [10000/10000 (100%)]\tLoss: 2.270162\n",
      "\n",
      "Test set: Average loss: 2.2685, Accuracy: 1031/10000 (10%)\n",
      "\n",
      "Train Epoch: 10 | With alice data |: [5000/10000 (50%)]\tLoss: 2.239307\n",
      "Train Epoch: 10 | With alice data |: [10000/10000 (100%)]\tLoss: 2.235741\n",
      "\n",
      "Test set: Average loss: 2.2340, Accuracy: 1814/10000 (18%)\n",
      "\n",
      "Train Epoch: 15 | With alice data |: [5000/10000 (50%)]\tLoss: 2.201066\n",
      "Train Epoch: 15 | With alice data |: [10000/10000 (100%)]\tLoss: 2.196833\n",
      "\n",
      "Test set: Average loss: 2.1945, Accuracy: 4024/10000 (40%)\n",
      "\n",
      "Train Epoch: 20 | With alice data |: [5000/10000 (50%)]\tLoss: 2.154058\n",
      "Train Epoch: 20 | With alice data |: [10000/10000 (100%)]\tLoss: 2.148707\n",
      "\n",
      "Test set: Average loss: 2.1454, Accuracy: 5467/10000 (55%)\n",
      "\n",
      "Train Epoch: 25 | With alice data |: [5000/10000 (50%)]\tLoss: 2.093801\n",
      "Train Epoch: 25 | With alice data |: [10000/10000 (100%)]\tLoss: 2.086833\n",
      "\n",
      "Test set: Average loss: 2.0818, Accuracy: 5943/10000 (59%)\n",
      "\n",
      "Train Epoch: 30 | With alice data |: [5000/10000 (50%)]\tLoss: 2.014391\n",
      "Train Epoch: 30 | With alice data |: [10000/10000 (100%)]\tLoss: 2.005089\n",
      "\n",
      "Test set: Average loss: 1.9972, Accuracy: 6481/10000 (65%)\n",
      "\n",
      "Train Epoch: 35 | With alice data |: [5000/10000 (50%)]\tLoss: 1.907391\n",
      "Train Epoch: 35 | With alice data |: [10000/10000 (100%)]\tLoss: 1.894793\n",
      "\n",
      "Test set: Average loss: 1.8826, Accuracy: 7194/10000 (72%)\n",
      "\n",
      "Train Epoch: 40 | With alice data |: [5000/10000 (50%)]\tLoss: 1.763199\n",
      "Train Epoch: 40 | With alice data |: [10000/10000 (100%)]\tLoss: 1.746455\n",
      "\n",
      "Test set: Average loss: 1.7284, Accuracy: 7663/10000 (77%)\n",
      "\n",
      "Train Epoch: 45 | With alice data |: [5000/10000 (50%)]\tLoss: 1.576812\n",
      "Train Epoch: 45 | With alice data |: [10000/10000 (100%)]\tLoss: 1.556071\n",
      "\n",
      "Test set: Average loss: 1.5312, Accuracy: 7949/10000 (79%)\n",
      "\n",
      "Train Epoch: 50 | With alice data |: [5000/10000 (50%)]\tLoss: 1.358983\n",
      "Train Epoch: 50 | With alice data |: [10000/10000 (100%)]\tLoss: 1.336545\n",
      "\n",
      "Test set: Average loss: 1.3060, Accuracy: 8216/10000 (82%)\n",
      "\n",
      "Train Epoch: 55 | With alice data |: [5000/10000 (50%)]\tLoss: 1.140511\n",
      "Train Epoch: 55 | With alice data |: [10000/10000 (100%)]\tLoss: 1.119990\n",
      "\n",
      "Test set: Average loss: 1.0869, Accuracy: 8391/10000 (84%)\n",
      "\n",
      "Train Epoch: 60 | With alice data |: [5000/10000 (50%)]\tLoss: 0.952763\n",
      "Train Epoch: 60 | With alice data |: [10000/10000 (100%)]\tLoss: 0.936318\n",
      "\n",
      "Test set: Average loss: 0.9035, Accuracy: 8524/10000 (85%)\n",
      "\n",
      "Train Epoch: 65 | With alice data |: [5000/10000 (50%)]\tLoss: 0.807668\n",
      "Train Epoch: 65 | With alice data |: [10000/10000 (100%)]\tLoss: 0.795408\n",
      "\n",
      "Test set: Average loss: 0.7644, Accuracy: 8626/10000 (86%)\n",
      "\n",
      "Train Epoch: 70 | With alice data |: [5000/10000 (50%)]\tLoss: 0.700671\n",
      "Train Epoch: 70 | With alice data |: [10000/10000 (100%)]\tLoss: 0.691691\n",
      "\n",
      "Test set: Average loss: 0.6628, Accuracy: 8692/10000 (87%)\n",
      "\n",
      "Train Epoch: 75 | With alice data |: [5000/10000 (50%)]\tLoss: 0.622056\n",
      "Train Epoch: 75 | With alice data |: [10000/10000 (100%)]\tLoss: 0.615409\n",
      "\n",
      "Test set: Average loss: 0.5886, Accuracy: 8767/10000 (88%)\n",
      "\n",
      "Train Epoch: 80 | With alice data |: [5000/10000 (50%)]\tLoss: 0.563368\n",
      "Train Epoch: 80 | With alice data |: [10000/10000 (100%)]\tLoss: 0.558335\n",
      "\n",
      "Test set: Average loss: 0.5334, Accuracy: 8806/10000 (88%)\n",
      "\n",
      "Train Epoch: 85 | With alice data |: [5000/10000 (50%)]\tLoss: 0.518394\n",
      "Train Epoch: 85 | With alice data |: [10000/10000 (100%)]\tLoss: 0.514471\n",
      "\n",
      "Test set: Average loss: 0.4913, Accuracy: 8851/10000 (89%)\n",
      "\n",
      "Train Epoch: 90 | With alice data |: [5000/10000 (50%)]\tLoss: 0.482931\n",
      "Train Epoch: 90 | With alice data |: [10000/10000 (100%)]\tLoss: 0.479793\n",
      "\n",
      "Test set: Average loss: 0.4583, Accuracy: 8889/10000 (89%)\n",
      "\n",
      "Train Epoch: 95 | With alice data |: [5000/10000 (50%)]\tLoss: 0.454195\n",
      "Train Epoch: 95 | With alice data |: [10000/10000 (100%)]\tLoss: 0.451612\n",
      "\n",
      "Test set: Average loss: 0.4316, Accuracy: 8944/10000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    train(args)\n",
    "    test(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! Here you are, you have trained a model on remote data using Federated Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PyGrid on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PyGrid](https://github.com/OpenMined/PyGrid)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PyGrid/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
